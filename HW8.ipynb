{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stuff=np.load(\"data.npz\")\n",
    "X_trn = stuff[\"X_trn\"]\n",
    "y_trn = stuff[\"y_trn\"]\n",
    "X_tst = stuff[\"X_tst\"]\n",
    "\n",
    "X_trn = np.float32(X_trn)\n",
    "y_trn = np.float32(y_trn)\n",
    "X_tst = np.float32(X_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "You will train several neural networks, each with a single hidden layer. These neural networks can be written as:\n",
    "\n",
    "$$\n",
    "f(x) = c + V \\sigma(b + W x).\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $x$ is the input, a vector of length $D$\n",
    "- $W$ is a matrix of size $M \\times D$ that maps input features to a hidden space\n",
    "- $b$ is the bias term for the hidden layer, a vector of length $M$\n",
    "- $\\sigma(a)=\\tanh(a)$ is the activation function. You will need that the derivative is $\\frac{d\\sigma(a)}{da} = 1-\\tanh(a)^2$.\n",
    "- $V$ is a matrix of size $O \\times M$ that maps the hidden space to the output space\n",
    "- $c$ is the bias term for the output space, a vector of length $O$\n",
    "\n",
    "Note that $f(x) : \\mathbb{R}^D\\rightarrow\\mathbb{R}^O$ is a function that maps a vector to a vector. We will refer to the $i$-the component of the output as $f(x)_i$.\n",
    "\n",
    "For this problem, we will use the logistic loss, defined as\n",
    "\n",
    "$$\n",
    "L(y,f) = -f_y + \\log \\sum_{i=0}^3 \\exp( f_i ),\n",
    "$$\n",
    "\n",
    "where $y \\in \\{0, 1, 2, 3\\}$ is the *label* for the input $x$, and $f \\in R^O$ is the output vector. Note that $f_y$ is therefore the $y^\\text{th}$ component of the output vector $f$. Also be careful to note that here, we are indexing $f$ from 0 instead of 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 1** (5 points) Write a function to evaluate the neural network and loss. Your function should have the following signature:\n",
    "\n",
    "```python\n",
    "def prediction_loss(x,y,W,V,b,c):\n",
    "    # do stuff here\n",
    "    return L\n",
    "```\n",
    "\n",
    "This should return a scalar. Give your function directly in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prediction_loss(W,V,b,c,x,y):\n",
    "    f = c + V @ np.tanh(b + W @ x)\n",
    "\n",
    "    log_term  = np.log(np.sum(np.exp(f))) \n",
    "    L = -f[y] + log_term\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 2** (10 points) Write a function to evaluate the gradient of the neural network. Your function should have the following signature. Do not use any packages outside of numpy.\n",
    "\n",
    "```python\n",
    "def prediction_grad(x,y,W,V,b,c):\n",
    "    # do stuff here\n",
    "    return dLdW, dLdV, dLdb, dLdc\n",
    "```\n",
    "\n",
    "Each returned array should be the same size as the input, and contain the corresponding gradient. So, for example,`dLdW` is the derivatives $\\nabla_W L(y,f(x))$. Give your function directly in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_grad(x, y, W, V, b, c):\n",
    "    # Forward pass\n",
    "    h = b + W @ x                  # hidden layer pre-activation\n",
    "    sigma_h = np.tanh(h)          # hidden layer activation\n",
    "    f = c + V @ sigma_h           # output\n",
    "    \n",
    "    # Compute softmax probabilities\n",
    "    exp_f = np.exp(f)\n",
    "    sum_exp_f = np.sum(exp_f)\n",
    "    probs = exp_f / sum_exp_f\n",
    "    \n",
    "    # Backward pass\n",
    "    # Start with df/df = probs - one_hot(y)\n",
    "    df = probs.copy()\n",
    "    df[y] -= 1\n",
    "    \n",
    "    # dL/dV = df/df * df/dV = df * sigma_h^T\n",
    "    dLdV = np.outer(df, sigma_h)\n",
    "    \n",
    "    # dL/dc = df/df * df/dc = df * 1\n",
    "    dLdc = df\n",
    "    \n",
    "    # dL/d(sigma_h) = df/df * df/d(sigma_h) = V^T * df\n",
    "    d_sigma_h = V.T @ df\n",
    "    \n",
    "    # dL/dh = dL/d(sigma_h) * d(sigma_h)/dh = d_sigma_h * (1 - tanh^2(h))\n",
    "    dh = d_sigma_h * (1 - sigma_h**2)\n",
    "    \n",
    "    # dL/dW = dL/dh * dh/dW = dh * x^T\n",
    "    dLdW = np.outer(dh, x)\n",
    "    \n",
    "    # dL/db = dL/dh * dh/db = dh * 1\n",
    "    dLdb = dh\n",
    "    \n",
    "    return dLdW, dLdV, dLdb, dLdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 3** (10 points) Take the following inputs, where there are 3 hidden units and 2 outputs ($y = 0 \\text{ or } y = 1$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x &= [1,2]\\\\\n",
    "y &= 1 \\\\\n",
    "W &= \\begin{pmatrix}0.5 & -1 \\\\ -0.5 & 1 \\\\ 1 & .5\\end{pmatrix}\\\\\n",
    "V &= \\begin{pmatrix}-1 & -1 & 1 \\\\ 1 & 1 & 1  \\end{pmatrix}\\\\\n",
    "b &= [0,0,0]\\\\\n",
    "c &= [0,0]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Run the function from the previous question to compute the gradient with respect to $W$, $V$, $b$, and $c$. Give the results directly in your report, organized as you see above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Grad W: [[-0.18070664 -0.36141328]\n",
      " [-0.18070664 -0.36141328]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      " Grad V: [[-0.45257413  0.45257413  0.48201379]\n",
      " [ 0.45257413 -0.45257413 -0.48201379]]\n",
      "\n",
      " Grad b: [-0.18070664 -0.18070664  0.        ]\n",
      "\n",
      " Grad c: [ 0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2])\n",
    "y = 1\n",
    "W = np.array([[0.5,-1], [-0.5,1], [1,0.5]])\n",
    "V = np.array([[-1,-1,1],[1,1,1]])\n",
    "b = np.array([0,0,0])\n",
    "c = np.array([0,0])\n",
    "\n",
    "results = prediction_grad(x,y,W,V,b,c)\n",
    "\n",
    "terms = ['W','V','b','c']\n",
    "\n",
    "for result, term in zip(results, terms):\n",
    "    print(f'\\n Grad {term}: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## JAX\n",
    "\n",
    "The next several questions use the `JAX` toolbox, which you can install via `pip install jax jaxlib`. The documentation for using grad from `JAX` can be found on this page:\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/quickstart.html#taking-derivatives-with-jax-grad\n",
    "\n",
    "\n",
    "**Question 4** (5 points) Write a function to evaluate the same gradient as in Question 2 using the Jax toolbox (Hint: You will need to import the NumPy wrapper, `import jax.numpy as np`, and the `grad` high-order function, `from jax import grad`).\n",
    "\n",
    "```python\n",
    "def prediction_grad_jax(x,y,W,V,b,c):\n",
    "    # do stuff here\n",
    "    return dLdW, dLdV, dLdb, dLdc\n",
    "```\n",
    "\n",
    "Give your function directly in your report. (You do not need to give any outputs from your function, but it is suggested to check the results against Q3 since if they are different, one must be wrong!)\n",
    "\n",
    "*Hint*: For JAX to work, the returned value from the loss function should be a scalar, you can use squeeze to ensure that. (e.g.  `L = L.squeeze()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "\n",
    "def prediction_grad_jax(W, V, b, c, x, y):\n",
    "    # Create the gradient function\n",
    "    grad_fn = grad(prediction_loss_jax, argnums=(0, 1, 2, 3))\n",
    "    # Call the gradient function with your inputs\n",
    "    return grad_fn(W, V, b, c, x, y)\n",
    "\n",
    "\n",
    "def prediction_loss_jax(W,V,b,c,x,y):\n",
    "    f = c + V @ jnp.tanh(b + W @ x)\n",
    "\n",
    "    log_term  = jnp.log(jnp.sum(jnp.exp(f))) \n",
    "    L = -f[y] + log_term\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad W: [[-0.18070672 -0.36141345]\n",
      " [-0.18070672 -0.36141345]\n",
      " [ 0.          0.        ]]\n",
      "Grad V: [[-0.4525741  0.4525741  0.4820138]\n",
      " [ 0.4525741 -0.4525741 -0.4820138]]\n",
      "Grad b: [-0.18070672 -0.18070672  0.        ]\n",
      "Grad c: [ 0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = np.array([1.0,2.0], dtype=np.float32)\n",
    "y = np.int32(1)\n",
    "\n",
    "W = np.array([[0.5,-1.0], \n",
    "              [-0.5,1.0], \n",
    "              [1.0,0.5]], dtype=np.float32)\n",
    "\n",
    "V = np.array([[-1.0,-1.0,1.0],\n",
    "\n",
    "              [1.0,1.0,1.0]], dtype=np.float32)\n",
    "b = np.array([0.0,0.0,0.0], dtype=np.float32)\n",
    "c = np.array([0.0,0.0], dtype=np.float32)\n",
    "\n",
    "grads = (prediction_grad_jax(W,V,b,c,x,y))\n",
    "\n",
    "dLdW, dLdV, dLdb, dLdc = prediction_grad_jax(W, V, b, c, x, y)\n",
    "print(f'Grad W: {dLdW}')\n",
    "print(f'Grad V: {dLdV}')\n",
    "print(f'Grad b: {dLdb}')\n",
    "print(f'Grad c: {dLdc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 5** (5 points) Update your function from Question 1. Instead of taking a single input x and a single output y, take an 2D of inputs X (where the first dimension indexes the different examples) and a 1D array of outputs Y. Also, take a regularization constant `λ` and apply squared regularization to `W` and `V`. Do not regularize `b` or `c`. Your function should be the sum of the logistic losses for each example in the dataset, plus the regularizer loss applied to `W` and `V`. (To be explicit, the regularizer could be written as $\\lambda \\left( \\sum_{vm} W_{vm}^2 + \\sum_{mi} V_{mi}^2 \\right)$.) Give your function directly in your report.\n",
    "\n",
    "```python\n",
    "def prediction_loss_full(X,Y,W,V,b,c,λ):\n",
    "    # do stuff here\n",
    "    return L # include regularization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def prediction_loss_full(X, Y, W, V, b, c, lam):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Loop through each example\n",
    "    for i in range(len(Y)):\n",
    "        # Forward pass\n",
    "        f = c + V @ jnp.tanh(b + W @ X[i])\n",
    "        \n",
    "        y_idx = jnp.asarray(Y[i], dtype=jnp.int32)\n",
    "        total_loss += -f[y_idx] + jnp.log(jnp.sum(jnp.exp(f)))  # Cast Y[i] to int for indexing\n",
    "    \n",
    "    # Add regularization\n",
    "    total_loss += lam * (jnp.sum(W**2) + jnp.sum(V**2))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 6** (5 points) ****Update your gradient function to work on a full dataset and include regularization, as in the previous question. Again, you should use JAX. Give your function directly in your report.\n",
    "\n",
    "```python\n",
    "def prediction_grad_full(X,Y,W,V,b,c,λ):\n",
    "    # do stuff here\n",
    "    return dLdW, dLdV, dLdb, dLdc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction_grad_full(X,Y,W,V,b,c,lam):\n",
    "    # Get gradients for all parameters using argnums\n",
    "    grads = grad(prediction_loss_full, argnums=(0, 1, 2, 3))(X,Y,W,V,b,c,lam)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 7** (15 points) Here is psuedo-code to optimize a function $h(w)$ by gradient descent with momentum.\n",
    "\n",
    "```python\n",
    "avg_grad = 0\n",
    "for iter = 1, 2, ... max_iters:\n",
    "    avg_grad = (1 - momentum) * avg_grad + momentum * ∇h(w)\n",
    "    w = w - stepsize * avg_grad\n",
    "```\n",
    "\n",
    "For each size of the hidden layer, $M \\in \\{5, 40, 70\\}$, train your neural network on the main data for this homework. Weights for layers $W$ and $V$ should be initialized by sampling from $\\frac{\\mathcal N(0, 1)}{\\sqrt{D}}$, where $\\mathcal N(0, 1)$ is the standard normal distribution and $D$ is the number of input dimensions for that layer. Weight for $b$ and $c$ should be initialized as zeros. \n",
    "\n",
    "Use gradient descent with momentum, with 1000 iterations, a step size of .000025, a momentum of 0.1, and $\\lambda = 1$.\n",
    "\n",
    "Report your code directly in the report, and the following:\n",
    "\n",
    "1. For each value of $M$, what is the total training time (in ms) for all iterations. (Give a table with 3 entries.)\n",
    "2. Make a plot of the training objective (regularized loss) as a function of iterations. This should be a single plot with 3 curves, one for each value of $M$. Include the plot in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network with M=5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (841,) and (3,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m M \u001b[38;5;129;01min\u001b[39;00m M_values:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining network with M=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     W, V, b, c, losses, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     training_times[M] \u001b[38;5;241m=\u001b[39m train_time\n\u001b[0;32m     15\u001b[0m     all_losses[M] \u001b[38;5;241m=\u001b[39m losses\n",
      "Cell \u001b[1;32mIn[16], line 66\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(X_trn, y_trn, M, max_iters, batch_size)\u001b[0m\n\u001b[0;32m     62\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Get gradients\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     dLdW, dLdV, dLdb, dLdc \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_grad_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Update momentum terms\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     avg_grad_W \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m momentum) \u001b[38;5;241m*\u001b[39m avg_grad_W \u001b[38;5;241m+\u001b[39m momentum \u001b[38;5;241m*\u001b[39m dLdW\n",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m, in \u001b[0;36mprediction_grad_full\u001b[1;34m(X, Y, W, V, b, c, lam)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_grad_full\u001b[39m(X, Y, W, V, b, c, lam):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Get gradients using JAX\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     grad_fn \u001b[38;5;241m=\u001b[39m grad(prediction_loss_full, argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mprediction_loss_full\u001b[1;34m(X, Y, W, V, b, c, lam)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_loss_full\u001b[39m(X, Y, W, V, b, c, lam):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b[\u001b[38;5;28;01mNone\u001b[39;00m, :]  \u001b[38;5;66;03m# [batch_size, M]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     sigma_h \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtanh(h)    \u001b[38;5;66;03m# [batch_size, M]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     f \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mdot(sigma_h, V\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m c[\u001b[38;5;28;01mNone\u001b[39;00m, :]  \u001b[38;5;66;03m# [batch_size, O]\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:8093\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m   8091\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   8092\u001b[0m     contract_dims \u001b[38;5;241m=\u001b[39m ((a_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (b_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m,))\n\u001b[1;32m-> 8093\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontract_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8094\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8095\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type,\n\u001b[0;32m   8097\u001b[0m                                           output_weak_type)\n",
      "    \u001b[1;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jason\\anaconda3\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:3117\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[1;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, algorithm, transpose_algorithm)\u001b[0m\n\u001b[0;32m   3114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[0;32m   3115\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3116\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3117\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[0;32m   3119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[1;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (841,) and (3,)."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "M_values = [3] \n",
    "D = X_trn.shape[1]\n",
    "O = y_trn.shape[0]\n",
    "\n",
    "avg_grad = 0\n",
    "step_size = 0.000025\n",
    "momentum = 0.1\n",
    "lam = 1\n",
    "max_iters = 100\n",
    "key = random.PRNGKey(0)\n",
    "training_losses = {M: [] for M in M_values}\n",
    "\n",
    "for M in M_values:\n",
    "    W = random.normal(key, shape=(M, D))\n",
    "    V = random.normal(key, shape=(O, M))\n",
    "    b = jnp.zeros([M, 1]) \n",
    "    c = jnp.zeros([O, 1])\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        dLdW, dLdV, dLdb, dLdc = prediction_grad_full(X_trn, y_trn, W, V, b, c, lam)\n",
    "        avg_grad = (1 - momentum) * avg_grad + momentum * dLdW\n",
    "        W -= step_size * avg_grad\n",
    "        loss = prediction_loss_full(X_trn, y_trn, W, V, b, c, lam)\n",
    "        training_losses[M].append(float(loss))\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time for M={M}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# plot training losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "for M in M_values:\n",
    "    plt.plot(training_losses[M], label=f'M={M}')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs Iterations for Different M')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Question 8** (10 points) Make a single train-validation split of the data with 50% used for training and 50% for testing. Train your neural network using the parameters above for each value of $M$ and give the estimated generalization error. Again, using the same initial weights generated using the scheme above. Then, retrain your network on *all* the data,  make predictions for the test data, and report your prediction for the test data (include in the `report_src/` folder). \n",
    "\n",
    "For the test data predictions, you will be required to submit a `.csv` file with two columns: an \"Id\" column and a \"Category\" column classifying the integer prediction for each element in `X_tst`. A sample solution using randomly predicted outputs can be generated as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def write_csv(y_pred, filename):\n",
    "    \"\"\"Write a 1d numpy array to a Kaggle-compatible .csv file\"\"\"\n",
    "    with open(filename, 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Id', 'Category'])\n",
    "        for idx, y in enumerate(y_pred):\n",
    "            csv_writer.writerow([idx, y])\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_tst = data['X_tst']\n",
    "y_pred = np.random.randint(0, 3, size=len(X_tst)) # random predictions\n",
    "write_csv(y_pred, 'submission.csv')\n",
    "```\n",
    "\n",
    "You can use the write_csv helper function in your code if you find it helpful to ensure that your solution is in the correct format.\n",
    "\n",
    "Report your code directly in the report, and the following:\n",
    "\n",
    "- What value of $M$ you chose.\n",
    "- The estimated generalization error.\n",
    "- The prediction on the test data (in a CSV file, named `submission.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
